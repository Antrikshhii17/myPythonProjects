					WEB SCRAPING

'''Snippet for site login before scraping'''
from bs4 import BeautifulSoup
import requests

# Start the session
session = requests.Session()

# Create the payload
payload = {'_username':'[YOUR_USERNAME]', 
          '_password':'[YOUR_PASSWORD]'
         }

# Post the payload to the site to log in
s = session.post("https://www.chess.com/login_check", data=payload)

# Navigate to the next page and scrape the data
s = session.get('https://www.chess.com/today')

soup = BeautifulSoup(s.text, 'html.parser')
soup.find('img')['src']
_________________________________________________________________________________________

'''Snippet for sending email alert for Bitcoin prices daily'''
from time import sleep

from requests import get
import schedule
from gmail import GMail, Message


def get_bitcoin_price():
    response = get('https://api.coindesk.com/v1/bpi/currentprice.json').json()
    return response['bpi']['USD']['rate']


def send_email(recipient: str='walid.mujahid.dev@gmail.com'):
    bitcoin_price = get_bitcoin_price()

    # enter actual password, otherwise, nothing happens.
    gmail = GMail('Price Alert <walid.mujahid.open@gmail.com>',
                  password='password')
    message = Message(f'Bitcoin is at {bitcoin_price} right now!',
                      to=recipient,
                      text=f'The current Bitcoin price is {bitcoin_price}.')

    gmail.send(message)


if __name__ == '__main__':
    schedule.every().day.at("06:30").do(send_email)

    while True:
        schedule.run_pending()
        sleep(1)
_________________________________________________________________________________________

					DATA VISUALIZATION 

OPERATION- To delete rows having age greater than 110.
CODE- df.drop(df[df['AGE'] > 110].index, inplace = True)


OPERATION- To plot histogram from dataframe having name and age columns
CODE- df1 = df.plot.hist(rwidth=0.9)
      df1.set_xlabel('Age')


OPERATION- To plot Male, female count coming from single column
CODE- df['gender'].value_counts().plot(kind='bar')
_________________________________________________________________________________________

					T-SQL
/*
Script to update the column where we need to join only capital letters from 
another column
*/


/*UPDATE md SET*/ SELECT sourceID, symbol = CONCAT('GDAPD',RIGHT(product,1),LEFT(sourceID,1),SUBSTRING(sourceID,CHARINDEX(' ',sourceID,1)+1,1)
  , SUBSTRING(sourceID,CHARINDEX(' ',sourceID,CHARINDEX(' ',sourceID)+1)+1,1))
  FROM MetaData md
  WHERE product LIKE 'GDAPT Day%'
  --AND symbol IS NULL
_________________________________________________________________________________________

					PANDAS

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', None)

_________________________________________________________________________________________

					PYTHON
# Read a specific file from a directory

directory = os.getcwd()+'<folder>'
specific_file = '<filename>'
print(directory)

dict_read = {}
if len(specific_file.split(",")) < 2:
    for file in os.listdir(directory):
        if file.endswith('.csv') and file.lower().startswith(specific_file):
            file_to_read = file
            print(file_to_read)

_________________________________________________________________________________________

					JSON

# Read data from nested URLs

import urllib.request
import json

base_url = '<url>'

def getdrugurl(base_url):
    # Getting only drug URL(s) from the base URL
    with urllib.request.urlopen(base_url) as reader:
        jsontext = reader.read().decode('utf-8')
        json_url_obj = json.loads(jsontext)
        drug_url = [*json_url_obj['formulary_urls']]
    return drug_url

each_url = getdrugurl(base_url)
each_url

def getproviderjson(drug_url):
    # Reading the JSON file from the URL
    with urllib.request.urlopen(drug_url) as reader:
        jsontext = reader.read().decode('utf-8')
        jsontext = jsontext.replace('false', '"False"').replace('true', '"True"')
        json_data = json.loads(jsontext)
    return json_data

json_data = getproviderjson(each_url)

json_data.toPandas().head(5)

_________________________________________________________________________________________

					HIVE
# Extract only numbers from a column

REGEXP_EXTRACT(<column_name>,'[0-9]+', 0)

_________________________________________________________________________________________

					DYNAMIC SQL IN PYTHON

# Build a dynamic select statement when columns are not known

# input.ini file content
--x--
[common]

dbname: <dbname>
tablename: <tablename>

count: 50
startdate : yyyy-mm-dd
enddate : yyyy-mm-dd

# Ensure you put NULL for every column that has no equivalent from source
[staging]

key1 : NULL
key2 : NULL
key3 : NULL
key4 : NULL
key5 : NULL
--x--


# To load the input.ini file into run_config dict
import configparser
run_config = {}
config = configparser.ConfigParser()
config.read('./input.ini')

for section in config.sections():
    for option in config.options(section):
        run_config[section+'.'+option] = config.get(section, option)


def dynamic_select_statement(spark, run_config):
    select_fields = ""
    
    for key, value in run_config.items():
        if 'staging' in key.lower():
            select_fields = select_fields + value + ", "
    
    select_sql = "Select " + select_fields + " from " + run_config['common.dbname'] + "." + run_config['common.tablename']
    select_sql = select_sql.replace(',  from',' from').replace(", )",")")
    
    print(select_sql)
    df = spark.sql(select_sql)
    df.write.insertInto(run_config['common.dbname'] + "." + <new_tablename>, overwrite = True)
    return None


# Use of stack for pivot

import pandas as pd

hhi_data = spark.sql("""SELECT * FROM <dbname>.<>tablename""")
l = []
unwanted_cols = ['<col1>','<col2>','<col3>']
seperator = ','
unpivot_columns = hhi_data.columns
unpivot_columns = [ele for ele in unpivot_columns if ele not in unwanted_cols]
unpivot_columns_helper=seperator.join(unpivot_columns).split(",")
for a in range(len(unpivot_columns)):
     l.append("'{}'".format(unpivot_columns[a]) + ",`" +unpivot_columns_helper[a]+"`")

unpivotExpr = "stack("+str(len(unpivot_columns))+","+ seperator.join(l)+") as (year,HHI)"
hhi_data_pivot = hhi_data.select("<column>", f.expr(unpivotExpr))

-------------------x---------------------------x-------------------------

# Snippet for gettign columns list from JSON metadata
with open('<json file>') as metadata:
    schema = json.load(metadata)
# Getting the column list from JSON metadata
cols, table = [], ''
for item in schema['extracts']:
    if file_to_read in item['fileName']:
        table = item['headerInfo']['tableName']
        for i in item['columnInfo']:
            cols.append(i)
metadata_cols = list(map(lambda x: x['columnName'], cols))

----------------x-----------------x----------------------x---------------

# Using regex to match year in column headers dynamically
if file_to_read == '<csv file>':
	curr_year, next_year = [], []
	countr = 0
	for item in metadata_cols:
		if any(char.isdigit() for char in item) and countr < 5:
			item = re.sub("\d+", str(int(year) - 1), item)
			curr_year.append(item)
			countr += 1
		elif any(char.isdigit() for char in item):
			item = re.sub("\d+", year, item)
			next_year.append(item)

---------------x-------------------------x------------------------x---------

# Separating static columns to merge with dynamic columns (cols containing year that changes in each file)
static_items = [item for item in metadata_cols if not any(char.isdigit() for char in item)]

---------------------x----------------------x--------------------------

# Function to unzip file recursively
def unzip_all(directory):
    # Performing unzip in recursion
    for item in os.listdir(directory):
        abs_path = os.path.join(directory, item)
        if item.endswith('.zip'):
            file_name = os.path.abspath(abs_path)
            zip_ref = zipfile.ZipFile(file_name, compression=zipfile.ZIP_DEFLATED)
            zip_ref.extractall(directory)
            zip_ref.close()
            os.remove(file_name)
        elif os.path.isdir(abs_path):
            unzip_all(abs_path)
    return None

-----------------x------------------------x-------------------------

# Scraper function
def get_data(url, year, directory):
    # Scraping the webpage using GET method of requests module
    page = requests.get(url)

    # Using bs4 to clean up the page
    soup = BeautifulSoup(page.content)
    soup.prettify()

    # Filtering the links ending with .zip
    zip_list = []
    for anchor in soup.findAll('a', href=True):
        links = anchor['href']
        if links.endswith('.zip') and year in links:
            zip_list.append(links)

    # Executing the wget to initiate the download
    for i in zip_list:
        zfile = i.split('/')[-1]
        urllib.request.urlretrieve(i, os.path.join(directory, zfile))
    return None

-------------------x---------------------------x-------------------------

# Perform column operation dynamically on a spark dataframe

df2 = df.select(*[f.col(c).alias(str(c).replace("yr_","")) for c in df.columns ])
df2 = df.select(*[f.col(c).alias(str(c).replace(' ','_')) if(' ') in str(c) else f.col(c) for c in df.columns])

lag_window = Window.partitionBy('<colname>').orderBy('<col2>')
df2 = df.select("*",*[f.lag(f.col(str(col)),1).over(lag_window).alias(str(col)+"_lagvalue") for col in df])

-------------------x---------------------------x-------------------------

# Count of flags with condition
report1 = report1_mid.groupBy('col1','col2').agg(f.countDistinct('col3').alias('dist_count_col3'),
                                               f.count(f.when(f.col('col4')==1,f.col('col4'))).alias('total_cnt_col4'),
                                               f.count(f.when(f.col('col5')==1,f.col('col5'))).alias('total_cnt_col5'),
                                               f.count(f.when(f.col('col6')==1,f.col('col6'))).alias('total_cnt_col6'))

-------------------x---------------------------x-------------------------

from boto3.session import Session
ACCESS_KEY='your_access_key'
SECRET_KEY='your_secret_key'

session = Session(aws_access_key_id=ACCESS_KEY,
                  aws_secret_access_key=SECRET_KEY)
s3 = session.resource('s3')
your_bucket = s3.Bucket('your_bucket')

for s3_file in your_bucket.objects.all():
    print(s3_file.key)

import boto3
s3 = boto3.resource('s3')
my_bucket = s3.Bucket("s3a://123456-abcd-gef")
for my_bucket_object in my_bucket.objects.all():
    print(my_bucket_object)

-------------------x---------------------------x-------------------------

# Read the config file
run_config = {}
config = configparser.ConfigParser()
config.read('teds_config_bcbsm_nebula.ini')

for section in config.sections():
    for option in config.options(section):
        run_config[section+'.'+option] = config.get(section, option)

-------------------x---------------------------x-------------------------

# Remove duplicates
df .groupby(['column1', 'column2']) \
.count() \
.where('count > 1') \
.sort('count', ascending=False).show()

-------------------x---------------------------x-------------------------
 
# Writing to Unity Catalog
catalog = "catalog_name"
schema = "schema_name"
table_name = "table_name"
df.write.format("delta").option("overwriteSchema", "true").mode("overwrite").saveAsTable(f"{catalog}.{schema}.{table_name}")

-------------------x---------------------------x-------------------------

# Rename columns of dataframe in single step
from functools import reduce
new_columns = ['col1', 'col2', 'col3', 'col4']
df_renamed = reduce(lambda df, x: df.withColumnRenamed(old_columns[x], new_columns[x]), range(len(new_columns)), df)

-------------------x---------------------------x-------------------------

# Reading data directly to spark dataframe
df = spark.read.format('csv').options(header='true', inferSchema='true', nanValue='').load('/Volumes/<volume_path>/<sample_data>.csv')

-------------------x---------------------------x-------------------------

# To wrap multiple elements of list in quotes where number of elements are not known
col2 = ("'" + "','".join(col3.split(",")) + "'")

-------------------x---------------------------x-------------------------

# Method to change date format from yyyymmdd to yyyy-mm-dd without string manipulation
from datetime import datetime
datesample = '20221031'
newdate = datetime.strptime(datesample, '%Y%m%d').date()
datenew = newdate.strftime('%Y-%m-%d')
datenew

-------------------x---------------------------x-------------------------

# When error says- location of the table already exists
df.write\
.option("path", "<table_path>")\
.mode("overwrite")\
.saveAsTable("dbname.tablename")

-------------------x---------------------------x-------------------------

# SUM the cols dynamically
cols = [df1.columns[-2], df1.columns[-1]]
df2 = df1.withColumn('newcol', f.expr('-'.join(cols)))

-------------------x---------------------------x-------------------------

# Pandas expand rows and columns
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', None)

-------------------x---------------------------x-------------------------

# Unpack elements from list of lists
data = list of lists
flat_list = [item for sublist in data for item in sublist]

-------------------x---------------------------x-------------------------

# Coalesce function in python
# def coalesce(*values):
#     """Return the first non-None value or None if all values are None"""
#     return next((v for v in values if v is not None), None)

-------------------x---------------------------x-------------------------

# Deduplication query in a nutshell
# test_query = spark.sql("select a.* from (select * from schema.table1 where <column> not in \
# (select distinct <column> from schema.table2 ) ) a where col2 = '<some_string>'")

-------------------x---------------------------x-------------------------

# Read excel fast in pandas
from xlsx2csv import Xlsx2csv
from io import StringIO
import pandas as pd

-------------------x---------------------------x-------------------------

# Function to read excel using read_csv
def read_excel(path: str, sheet_name: str) -> pd.DataFrame:
    buffer = StringIO()
    Xlsx2csv(path, outputencoding="utf-8", sheet_name=sheet_name).convert(buffer)
    buffer.seek(0)
    df = pd.read_csv(buffer)
    return df

-------------------x---------------------------x-------------------------

# Fix df.collect() output
coltest3 = spark.sql("show columns in schema.table").collect()
coltest2 = [row.col_name for row in coltest3 if '<specific_col>' not in row]
coltest = ', '.join(coltest2)
coltest

-------------------x---------------------------x-------------------------

# Make first list element as column header before converting to pddf
class_dict = {k[0]: list(k) for i, k in enumerate(class_list)}
class_df = pd.DataFrame.from_dict(class_dict, orient='index').transpose()

-------------------x---------------------------x-------------------------


